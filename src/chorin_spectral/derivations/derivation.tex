\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}

\title{Chorin's Projection Method with Spectral Collocation for 2D Incompressible Navier Stokes}
\maketitle

The setup and time discretization with Crank Nicholson remains the same as the finite difference version of this (see other directory in repo). Here, we focus on deriving a Chebyshev pseudo-spectral estimator.

Recall the NSE (with no force function)
\begin{align*}
    \frac{\partial \mathbf{u}}{\partial t} + (\mathbf{u}\cdot \nabla)\mathbf{u} + \nabla p &= \Delta \mathbf{u} \\
    \nabla \cdot \mathbf{u} = 0 \\
    \mathbf{u} = 0 \quad\text{on}\quad \partial \Omega
\end{align*}

\paragraph{Chorin: Step 1} Also recall that the Chorin's projection method first ignores pressure to comute an intermediate velocity field
\begin{align*}
    \frac{\partial \mathbf{u}^*}{\partial t} + (\mathbf{u}\cdot \nabla)\mathbf{u} &= \Delta \mathbf{u}^* \\
    \mathbf{u}^* &= 0 \quad\text{on}\quad \partial \Omega
\end{align*}

We discretize time first with Adams-Bashford and implicit Crank-Nicholson

\begin{equation*}
    \frac{\mathbf{u}^* - \mathbf{u}^n}{\bigtriangleup t} + (\mathbf{u}^{\frac{n+1}{2}} \cdot \nabla) \mathbf{u}^{\frac{n+1}{2}} = \Delta \mathbf{u}^{\frac{n+1}{2}}
\end{equation*}

So far things are identical to the finite difference version of Chorin's. But to solve this, we do not discretize spatial dimensions anymore: we use (pseudo)spectral methods:

We want to approximate the solution $\mathbf{u}^*$ as truncated series of Chebyshev polynomials: $\{ T_k(x) \}_{k=1}^{\infty}$ where $T_k(x) = \cos(k \cos^{-1}x)$; each polynomial is restricted to $[-1, 1]$.

\begin{align*}
    u^*_{N}(x) &= \sum_{k=0}^{\infty} \hat{u}^*_k T_k(x) \approx \sum_{k=0}^{N} \hat{u}^*_k T_k(x)
\end{align*}

where $\mathbf{u}^* = (u^*, v^*)$. We just write the derivation for the first equation for simplicity. To get the Chebyshev coefficients, we have to compute the (normalized) inner product:

\begin{align*}
    \hat{u}_k &= \frac{2}{\pi c_k} \int_{-1}^1 u T_k w dx
\end{align*}
where $c_k = \left\{\begin{matrix}
2 &  \text{if} \quad k = 0\\
1 &  \text{if} \quad k \geq 1
\end{matrix}\right\}$. This is hard to compute, so we need to estimate the integral. Normally, this would require a lot of points but we can get away with carefully selected ones and a Gaussian quadrature. The best points turn out to be the roots of another Chebyshev polynomial with degree one higher; these are called the Gauss-Lobatto points -- they end up with denser spread near the boundaries (-1 and 1).
\begin{equation}
    x_i = \cos \frac{\pi i}{N}, i = 0, ..., N
\end{equation}
resulting in,
\begin{equation}
    \hat{u}_k = \frac{2}{\bar{c}_k N} \sum_{i=0}^N \frac{1}{\bar{c}_i} u_i T_k(x_i), k = 0, ..., N
\end{equation}
where $\bar{c}_k = \left\{\begin{matrix}
2 &  \text{if} \quad k = 0 \\
1 &  \text{if} \quad 1 \leq k \leq N - 1 \\
2 &  \text{if} \quad k = N \\
\end{matrix}\right\}$. Note that to convert back and forth between spectral coefficients ($\hat{u}_k$) and the values at the collocation points ($u_N(x_i)$) is a matrix multiplication. To be explicit, let
$\mathcal{T} = [cos k\pi i / N], k, i = 0, ..., N$ and $\mathcal{T}^{-1} = [2(\cos \pi i / N) / (\bar{c}_k \bar{c}_i N)]$. Then,

\begin{align*}
    \mathcal{U}^* &= \mathcal{T}\hat{\mathcal{U}^*} \\
    \hat{\mathcal{U}}^* &= \mathcal{T}\mathcal{U}^*
\end{align*}
where $\mathcal{U}^* = [u^*(x_0), ..., u^*(x_N)]$ and $\hat{\mathcal{U}}^* = [\hat{u}_0, ..., \hat{u}_N]$.

An interesting fact that is useful is that the  approximation:

\begin{equation}
    u^*_{N}(x) = \sum_{k=0}^{N} \hat{u}^*_k T_k(x)
\end{equation}

can be viewed as a Lagrange interpolating polynomial with a set $\{x_i\}$. One can explicitly write this as

\begin{equation}
    u^*_{N}(x) = \sum_{j=0}^{N} h_j(x)u^*(x_j)
\end{equation}
where $h_j(x) = \frac{(-1)^{j+1}(1-x^2)T'_N(x)}{\bar{c}_j N^2 (x - x_j)}$ is some crazy polynomial. This is really useful because it lets us do differentiation in closed form in physical space (no need for fourier transforms). In particular, we can write the $p$-th derivative,
\begin{equation}
    u^{*, (p)}_{N}(x_i) = \sum_{k=0}^N \hat{u}_k T^{(p)}_k (x_i) = \sum_{j=0}^N h^{(p)}_j(x_i)u_N(x_j)
\end{equation}
Notice then that computing the derivative is just a matrix multiplication on the existing coordinate values! If we let $d_{i,j}^{(p)} = h_j^{(p)}(x_i)$, then we can get a series of formulas populating a ``derivative matrix", $\mathcal{D} = [d^{(1)}_{i,j}], i, j = 0 , ...,N$.

\begin{align*}
    d_{i,j}^{(1)} &= \frac{\bar{c}_i}{\bar{c}_j}\frac{(-1)^{i+j}}{(x_i - x_j)}, 0 \leq i,j \leq N , i \neq j\\
    d_{i,i}^{(1)} &= -\frac{x_i}{2(1-x_i^2)}, 1\leq i \leq N - 1\\
    d_{0,0}^{(1)} &= -d_{N, N}^{(1)} = \frac{2N^2 + 1}{6}
\end{align*}

So, in short, $\mathcal{U}^{*, (1)} = \mathcal{D}\mathcal{U}^{*}$,  $\mathcal{U}^{*, (2)} = \mathcal{D}^2\mathcal{U}^{*}$. As nice as this is,there are a few hacks we need to be careful of for numerical stability.

First, calculating $(1-x_i^2)$ and $(x_i - x_j)$ may be hard if points are very close, so we use the following:

\begin{align*}
    x_i - x_j &= 2\sin \frac{(j+i)\pi}{2N}\sin \frac{(j-i)\pi}{2N} \\
    1 - x_i^2 &= \sin^2 \frac{i \pi}{N}
\end{align*}

Second, since this differentiation matrix is approximated, it doesn't always represent the derivative of a constant (which it should). In other words, it should be that
\begin{equation}
    \sum_{j=0}^N d_{i,j}^{(1)} = 0, i = 0, ..., N
\end{equation}
. To fix this, we should calculate the off diagonal entries later to satisfy this constraint. So...
\begin{equation}
    d_{i,j}^{(1)} = -\sum_{j=0, j\neq i}^N d_{i,j}^{(1)}, i = 0, ..., N
\end{equation}

When we want to compute $\mathcal{D}$ and $\mathcal{D}^2$, we use the following procedure:
\begin{itemize}
    \item compute $\mathcal{D}$ with the diagonal term correction
    \item square it to get a provisional $\tilde{\mathcal{D}}^2$.
    \item correct $\tilde{\mathcal{D}}^2$ to nsum to 1. As in first set $d^{(2)}_{i,j} = \tilde{d}^{(2)}_{i,j}$ for $j \neq i$. Then set $d^{(2)}_{i,i} = -\sum_{j=0, j\neq i}^{N} \tilde{d}^{(2)}_{i,j}, i = 0,..., N$.
\end{itemize}

Ok, now that we know how to differentiate, we revisit the original equation. The intuition is that we can replace all unknown values in terms of the coordinate values. Then there's a system of equations that we need to matrix diagonalize.

\end{document}